{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyORghvNKxyY27s/3vbxV1Gi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyasingh10oct/MLAI/blob/main/PDF_chat_using_llama2_CPP_newPineconeAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Install All the Required Pakages (Llama2 + Langchain + Pinecone)"
      ],
      "metadata": {
        "id": "mJjx_JSjYseG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICVzYdcKWo11",
        "outputId": "d1e15891-d5a3-43c4-e942-e0248b235ac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.17)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.18)\n",
            "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.86)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.14)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.0.1)\n",
            "Requirement already satisfied: unstructured in /usr/local/lib/python3.10/dist-packages (0.12.3)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.10.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.4)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured) (2024.1.2)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.26.4)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.6.1)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.0)\n",
            "Requirement already satisfied: unstructured-client>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (2023.11.17)\n",
            "Requirement already satisfied: charset-normalizer>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (3.3.2)\n",
            "Requirement already satisfied: dataclasses-json-speakeasy>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (0.5.11)\n",
            "Requirement already satisfied: idna>=3.4 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (3.6)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (1.0.6)\n",
            "Requirement already satisfied: marshmallow>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (3.20.2)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (1.0.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (2.8.2)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (1.16.0)\n",
            "Requirement already satisfied: typing-inspect>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (0.9.0)\n",
            "Requirement already satisfied: urllib3>=1.26.18 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (2.0.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.1)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2023.11.17)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.9.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.38)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.9.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install pypdf\n",
        "!pip install unstructured\n",
        "!pip install sentence_transformers\n",
        "!pip install pinecone-client\n",
        "!pip install llama-cpp-python\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Query the Docs to get the Answer Back (Llama 2 Model)\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEtKMBOPbjvw",
        "outputId": "c6ca3ce9-0c0d-4df0-8c76-0fe938c1ea8d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.38.tar.gz (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Running command pip subprocess to install build dependencies\n",
            "  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "  Collecting scikit-build-core[pyproject]>=0.5.1\n",
            "    Using cached scikit_build_core-0.8.0-py3-none-any.whl (139 kB)\n",
            "  Collecting exceptiongroup (from scikit-build-core[pyproject]>=0.5.1)\n",
            "    Using cached exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
            "  Collecting packaging>=20.9 (from scikit-build-core[pyproject]>=0.5.1)\n",
            "    Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
            "  Collecting tomli>=1.1 (from scikit-build-core[pyproject]>=0.5.1)\n",
            "    Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "  Collecting pathspec>=0.10.1 (from scikit-build-core[pyproject]>=0.5.1)\n",
            "    Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "  Collecting pyproject-metadata>=0.5 (from scikit-build-core[pyproject]>=0.5.1)\n",
            "    Using cached pyproject_metadata-0.7.1-py3-none-any.whl (7.4 kB)\n",
            "  Installing collected packages: tomli, pathspec, packaging, exceptiongroup, scikit-build-core, pyproject-metadata\n",
            "  ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "  lida 0.0.10 requires fastapi, which is not installed.\n",
            "  lida 0.0.10 requires kaleido, which is not installed.\n",
            "  lida 0.0.10 requires python-multipart, which is not installed.\n",
            "  lida 0.0.10 requires uvicorn, which is not installed.\n",
            "  Successfully installed exceptiongroup-1.2.0 packaging-23.2 pathspec-0.12.1 pyproject-metadata-0.7.1 scikit-build-core-0.8.0 tomli-2.0.1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Getting requirements to build wheel\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command pip subprocess to install backend dependencies\n",
            "  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "  Collecting cmake>=3.21\n",
            "    Using cached cmake-3.28.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.3 MB)\n",
            "  Collecting ninja>=1.5\n",
            "    Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "  Installing collected packages: ninja, cmake\n",
            "    Creating /tmp/pip-build-env-1iiohhti/normal/local/bin\n",
            "    changing mode of /tmp/pip-build-env-1iiohhti/normal/local/bin/ninja to 755\n",
            "    changing mode of /tmp/pip-build-env-1iiohhti/normal/local/bin/cmake to 755\n",
            "    changing mode of /tmp/pip-build-env-1iiohhti/normal/local/bin/cpack to 755\n",
            "    changing mode of /tmp/pip-build-env-1iiohhti/normal/local/bin/ctest to 755\n",
            "  Successfully installed cmake-3.28.1 ninja-1.11.1.1\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "  *** scikit-build-core 0.8.0 using CMake 3.28.1 (metadata_wheel)\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m225.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2>=2.11.3 (from llama-cpp-python)\n",
            "  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m309.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
            "  *** scikit-build-core 0.8.0 using CMake 3.28.1 (wheel)\n",
            "  *** Configuring CMake...\n",
            "  loading initial cache file /tmp/tmp1z3z2_g5/build/CMakeInit.txt\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "  -- Found Threads: TRUE\n",
            "  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.2.140\")\n",
            "  -- cuBLAS found\n",
            "  -- The CUDA compiler identification is NVIDIA 12.2.140\n",
            "  -- Detecting CUDA compiler ABI info\n",
            "  -- Detecting CUDA compiler ABI info - done\n",
            "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "  -- Detecting CUDA compile features\n",
            "  -- Detecting CUDA compile features - done\n",
            "  -- Using CUDA architectures: 52;61;70\n",
            "  -- CUDA host compiler is GNU 11.4.0\n",
            "\n",
            "  -- Warning: ccache not found - consider installing it or use LLAMA_CCACHE=OFF\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  CMake Warning (dev) at CMakeLists.txt:21 (install):\n",
            "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  CMake Warning (dev) at CMakeLists.txt:30 (install):\n",
            "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  -- Configuring done (3.6s)\n",
            "  -- Generating done (0.0s)\n",
            "  -- Build files have been written to: /tmp/tmp1z3z2_g5/build\n",
            "  *** Building project with Ninja...\n",
            "  Change Dir: '/tmp/tmp1z3z2_g5/build'\n",
            "\n",
            "  Run Build Command(s): /tmp/pip-build-env-1iiohhti/normal/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -v\n",
            "  [1/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -c /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/ggml-alloc.c\n",
            "  [2/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -c /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/ggml-backend.c\n",
            "  [3/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -c /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/ggml-quants.c\n",
            "  [4/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -c /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/ggml.c\n",
            "  [5/23] cd /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp && /tmp/pip-build-env-1iiohhti/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -DMSVC= -DCMAKE_C_COMPILER_VERSION=11.4.0 -DCMAKE_C_COMPILER_ID=GNU -DCMAKE_VS_PLATFORM_NAME= -DCMAKE_C_COMPILER=/usr/bin/cc -P /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/common/../scripts/gen-build-info-cpp.cmake\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  [6/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600  -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -c /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/common/build-info.cpp\n",
            "  [7/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/common/. -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/common/common.cpp\n",
            "  [8/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/common/. -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -c /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/common/sampling.cpp\n",
            "  [9/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/common/. -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/common/console.cpp\n",
            "  [10/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/common/. -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -c /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/common/grammar-parser.cpp\n",
            "  [11/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/common/. -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -c /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/common/train.cpp\n",
            "  [12/23] /usr/bin/c++ -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -c /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/examples/llava/llava.cpp\n",
            "  [13/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/llama.cpp\n",
            "  [14/23] /usr/bin/c++  -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/common/. -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/. -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/examples/llava/../../common -O3 -DNDEBUG -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -c /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/examples/llava/llava-cli.cpp\n",
            "  [15/23] /usr/bin/c++ -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -c /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/examples/llava/clip.cpp\n",
            "  [16/23] : && /tmp/pip-build-env-1iiohhti/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/examples/llava/libllava_static.a && /usr/bin/ar qc vendor/llama.cpp/examples/llava/libllava_static.a  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o && /usr/bin/ranlib vendor/llama.cpp/examples/llava/libllava_static.a && :\n",
            "  [17/23] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -use_fast_math -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o.d -x cu -c /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/ggml-cuda.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
            "  /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/ggml-cuda.cu(645): warning #177-D: function \"warp_reduce_sum(half2)\" was declared but never referenced\n",
            "                                     half2 warp_reduce_sum(half2 a) {\n",
            "                                           ^\n",
            "\n",
            "  Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "  /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/ggml-cuda.cu(666): warning #177-D: function \"warp_reduce_max(half2)\" was declared but never referenced\n",
            "                                     half2 warp_reduce_max(half2 x) {\n",
            "                                           ^\n",
            "\n",
            "  /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/vendor/llama.cpp/ggml-cuda.cu(1696): warning #177-D: variable \"ksigns64\" was declared but never referenced\n",
            "                           uint64_t ksigns64[128] = {\n",
            "                                    ^\n",
            "\n",
            "  [18/23] : && /tmp/pip-build-env-1iiohhti/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/libggml_static.a && /usr/bin/ar qc vendor/llama.cpp/libggml_static.a  vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o && /usr/bin/ranlib vendor/llama.cpp/libggml_static.a && :\n",
            "  [19/23] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllama.so -o vendor/llama.cpp/libllama.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -L/usr/local/cuda/targets/x86_64-linux/lib -Wl,-rpath,/usr/local/cuda-12.2/targets/x86_64-linux/lib:  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl && :\n",
            "  [20/23] : && /usr/bin/g++ -fPIC  -shared -Wl,-soname,libggml_shared.so -o vendor/llama.cpp/libggml_shared.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl -L\"/usr/local/cuda/targets/x86_64-linux/lib/stubs\" -L\"/usr/local/cuda/targets/x86_64-linux/lib\" && :\n",
            "  [21/23] : && /tmp/pip-build-env-1iiohhti/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/common/libcommon.a && /usr/bin/ar qc vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o && /usr/bin/ranlib vendor/llama.cpp/common/libcommon.a && :\n",
            "  [22/23] : && /usr/bin/c++ -O3 -DNDEBUG  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -o vendor/llama.cpp/examples/llava/llava-cli  -Wl,-rpath,/tmp/tmp1z3z2_g5/build/vendor/llama.cpp:/usr/local/cuda-12.2/targets/x86_64-linux/lib:  vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/libllama.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so && :\n",
            "  [23/23] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllava.so -o vendor/llama.cpp/examples/llava/libllava.so vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o  -Wl,-rpath,/tmp/tmp1z3z2_g5/build/vendor/llama.cpp:/usr/local/cuda-12.2/targets/x86_64-linux/lib:  vendor/llama.cpp/libllama.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a  /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so && :\n",
            "\n",
            "  *** Installing project into wheel...\n",
            "  -- Install configuration: \"Release\"\n",
            "  -- Installing: /tmp/tmp1z3z2_g5/wheel/platlib/lib/libggml_shared.so\n",
            "  -- Installing: /tmp/tmp1z3z2_g5/wheel/platlib/lib/cmake/Llama/LlamaConfig.cmake\n",
            "  -- Installing: /tmp/tmp1z3z2_g5/wheel/platlib/lib/cmake/Llama/LlamaConfigVersion.cmake\n",
            "  -- Installing: /tmp/tmp1z3z2_g5/wheel/platlib/include/ggml.h\n",
            "  -- Installing: /tmp/tmp1z3z2_g5/wheel/platlib/include/ggml-alloc.h\n",
            "  -- Installing: /tmp/tmp1z3z2_g5/wheel/platlib/include/ggml-backend.h\n",
            "  -- Installing: /tmp/tmp1z3z2_g5/wheel/platlib/include/ggml-cuda.h\n",
            "  -- Installing: /tmp/tmp1z3z2_g5/wheel/platlib/lib/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmp1z3z2_g5/wheel/platlib/lib/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/tmp1z3z2_g5/wheel/platlib/include/llama.h\n",
            "  -- Installing: /tmp/tmp1z3z2_g5/wheel/platlib/bin/convert.py\n",
            "  -- Installing: /tmp/tmp1z3z2_g5/wheel/platlib/bin/convert-lora-to-ggml.py\n",
            "  -- Installing: /tmp/tmp1z3z2_g5/wheel/platlib/llama_cpp/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmp1z3z2_g5/wheel/platlib/llama_cpp/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/llama_cpp/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/llama_cpp/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/tmp1z3z2_g5/wheel/platlib/lib/libllava.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmp1z3z2_g5/wheel/platlib/lib/libllava.so\" to \"\"\n",
            "  -- Installing: /tmp/tmp1z3z2_g5/wheel/platlib/bin/llava-cli\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmp1z3z2_g5/wheel/platlib/bin/llava-cli\" to \"\"\n",
            "  -- Installing: /tmp/tmp1z3z2_g5/wheel/platlib/llama_cpp/libllava.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmp1z3z2_g5/wheel/platlib/llama_cpp/libllava.so\" to \"\"\n",
            "  -- Installing: /tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/llama_cpp/libllava.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-vf6prxnv/llama-cpp-python_9f843e80ae254024adacba8c71d76e3c/llama_cpp/libllava.so\" to \"\"\n",
            "  *** Making wheel...\n",
            "  *** Created llama_cpp_python-0.2.38-cp310-cp310-manylinux_2_35_x86_64.whl...\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.38-cp310-cp310-manylinux_2_35_x86_64.whl size=9697581 sha256=7d9ec447ab113ba7de4da809b2b472aa39747879c30f41f3379d0660e59caefa\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ucnerpdn/wheels/eb/58/77/20d3d9a235b4930050fbcde1ad4f0a4d054644269e801b08aa\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.9.0\n",
            "    Uninstalling typing_extensions-4.9.0:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.9.0.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n",
            "      Successfully uninstalled typing_extensions-4.9.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Removing file or directory /usr/local/bin/f2py\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-1.26.4.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  changing mode of /usr/local/bin/f2py to 755\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.5\n",
            "    Uninstalling MarkupSafe-2.1.5:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/MarkupSafe-2.1.5.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/markupsafe/\n",
            "      Successfully uninstalled MarkupSafe-2.1.5\n",
            "  Attempting uninstall: diskcache\n",
            "    Found existing installation: diskcache 5.6.3\n",
            "    Uninstalling diskcache-5.6.3:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache-5.6.3.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache/\n",
            "      Successfully uninstalled diskcache-5.6.3\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.3\n",
            "    Uninstalling Jinja2-3.1.3:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/Jinja2-3.1.3.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/jinja2/\n",
            "      Successfully uninstalled Jinja2-3.1.3\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama_cpp_python 0.2.38\n",
            "    Uninstalling llama_cpp_python-0.2.38:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/bin/__pycache__/convert-lora-to-ggml.cpython-310.pyc\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/bin/__pycache__/convert.cpython-310.pyc\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/bin/convert-lora-to-ggml.py\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/bin/convert.py\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/bin/llava-cli\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/include/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/lib/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/llama_cpp/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/llama_cpp_python-0.2.38.dist-info/\n",
            "      Successfully uninstalled llama_cpp_python-0.2.38\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.3 llama-cpp-python-0.2.38 numpy-1.26.4 typing-extensions-4.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2: Import all required libraries**"
      ],
      "metadata": {
        "id": "hl6zSoCLYqT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader, OnlinePDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Pinecone as PineconeStore\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "import pinecone\n",
        "import os"
      ],
      "metadata": {
        "id": "7Ns_EkNAZHU6"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3: Load the Data**"
      ],
      "metadata": {
        "id": "j0MbFhxtZLbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"/content/Wells Fargo - Dataset.pdf\")\n",
        "data = loader.load()\n",
        "#data"
      ],
      "metadata": {
        "id": "ViQyzBbhZSbb"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Split the Text into **Chunks**"
      ],
      "metadata": {
        "id": "yNoNUG9PcIQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)"
      ],
      "metadata": {
        "id": "mWZ7qIOUcLdh"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs=text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "Hx1DnnLocSxB"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "El4zjm53cViA",
        "outputId": "d6fe33d8-2e6b-499d-8e51-4c64cfe624da"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2675"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Pr2KkypcgHV",
        "outputId": "ccc7357d-53d7-49a6-adf7-f77836931a42"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='The above actions build on services we’ve introduced over the past several years, including Clear Access Banking, a consumer banking account with no overdraft fees. We now have over 1.7 million of those accounts, up 48% from a year ago. \\nBeyond our customers, we have an important responsibility to \\nstrengthen the communities we serve. We continued that work in 2022. This was true on several fronts. \\nSupporting homeownership \\n•\\t We launched a Special Purpose Credit Program in 2022,', metadata={'source': '/content/Wells Fargo - Dataset.pdf', 'page': 7})"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5: Setup the Environment**"
      ],
      "metadata": {
        "id": "CbVUvRM8cyv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', '5fd5141c-3765-46cd-a14e-f5576041c1b9')\n",
        "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'gcp-starter')\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"5fd5141c-3765-46cd-a14e-f5576041c1b9\"\n",
        "os.environ[\"PINECONE_API_ENV\"] = \"gcp-starter\""
      ],
      "metadata": {
        "id": "A-Tb3AwIc0qv"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 6: Downlaod the Embeddings**"
      ],
      "metadata": {
        "id": "8NC8_2A1c905"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "1E-ZOBQcc8FO"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Initializing the **Pinecone**"
      ],
      "metadata": {
        "id": "AFMV12pAdGUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Old API\n",
        "# initialize pinecone\n",
        "#pinecone.init(\n",
        "    #api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
        "    #environment=PINECONE_API_ENV  # next to api key in console\n",
        "#)\n",
        "#index_name = \"langchainpinecone\" # put in the name of your pinecone index here"
      ],
      "metadata": {
        "id": "Crp_1rZwWZXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize pinecone\n",
        "from pinecone import Pinecone\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index_name = \"test\" # put in the name of your pinecone index here\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "W6AImGbQdH83"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 8: Create Embeddings for Each of the Text Chunk**"
      ],
      "metadata": {
        "id": "zStThgJQdOe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch=PineconeStore.from_texts([t.page_content for t in docs], embeddings, index_name=\"test\")\n",
        "\n",
        "#If you already have an index, you can load it like this\n",
        "#docsearch = Pinecone.from_existing_index(index_name, embeddings)"
      ],
      "metadata": {
        "id": "MZ_IeHv_dQ1t"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 9: Similarity Search**"
      ],
      "metadata": {
        "id": "aDNgdGWpfVkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#query=\"Who is the Chief Executive Officer and President of Wells Fargo & Company?\"\n",
        "query=\"What was the net income of Wells Fargo in 2022?\""
      ],
      "metadata": {
        "id": "ASnz-ClefXF2"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs=docsearch.similarity_search(query, k=3)"
      ],
      "metadata": {
        "id": "XD_LMjiwfabq"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i8DgQHUfe3w",
        "outputId": "0b70196b-3a63-4e7c-c9ae-d035a5518327"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted'),\n",
              " Document(page_content='Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted'),\n",
              " Document(page_content='Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted')]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 9: Query the Docs to get the Answer Back (Llama 2 Model)**"
      ],
      "metadata": {
        "id": "WZRzzfAQftPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose"
      ],
      "metadata": {
        "id": "_8pVKD49gKyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import All the Required Libraries**"
      ],
      "metadata": {
        "id": "s-ryh1U9g3bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ],
      "metadata": {
        "id": "4_NkQ-y6g48V"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "# Verbose is required to pass to the callback manager"
      ],
      "metadata": {
        "id": "t6JHWVUPg9Nb"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantized Models from the Hugging Face Community**\n",
        "The Hugging Face community provides quantized models, which allow us to efficiently and effectively utilize the model on the T4 GPU. It is important to consult reliable sources before using any model.\n",
        "\n",
        "There are several variations available, but the ones that interest us are based on the GGLM library.\n",
        "\n",
        "We can see the different variations that Llama-2-13B-GGML has here.\n",
        "\n",
        "In this case, we will use the model called Llama-2-13B-chat-GGML.\n",
        "\n",
        "Quantization reduces precision to optimize resource usage.\n",
        "\n",
        "Quantization is a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer ( int8 ) instead of the usual 32-bit floating point ( float32 )."
      ],
      "metadata": {
        "id": "jiE_o7QhhBB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model_name_or_path = \"TheBloke/CodeLlama-13B-Python-GGUF\"\n",
        "#model_basename = \"codellama-13b-python.Q5_K_M.gguf\" # the model is in bin format\n",
        "\n",
        "model_name_or_path = \"TheBloke/CodeLlama-7B-Python-GGUF\"\n",
        "model_basename = \"codellama-7b-python.Q5_K_M.gguf\" # the model is in bin format"
      ],
      "metadata": {
        "id": "0PbKdc8UhESq"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this before the demo, the download takes time\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ],
      "metadata": {
        "id": "vFlynabYiq5b"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
        "n_batch = 256  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "\n",
        "# Loading model,\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path,\n",
        "    max_tokens=256,\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    callback_manager=callback_manager,\n",
        "    n_ctx=1024,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_b02t4VivV0",
        "outputId": "b7b557e5-3338-4eb6-ce72-9227938c9056"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '16384', 'general.name': 'codellama_codellama-7b-python-hf', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain=load_qa_chain(llm, chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "HNWCNxU_iyuI"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#query=\"What is the fullform of HTM?\"\n",
        "query=\"What was the net income of Wells Fargo in 2022?\"\n",
        "docs=docsearch.similarity_search(query)"
      ],
      "metadata": {
        "id": "GIPn0Huwi1U6"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmZifPF7i3K1",
        "outputId": "350db7f2-55d8-4830-fcce-e5fdd74ca5dc"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted'),\n",
              " Document(page_content='Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted'),\n",
              " Document(page_content='Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted'),\n",
              " Document(page_content='Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted')]"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "Lk3Hpj87i7Sz",
        "outputId": "fc07344e-3fee-4f8b-e7de-d694483eefd5"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted).\n",
            "Rogue Answer: Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted)).\n",
            "Bad Answer: Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted)).\n",
            "Misleading Answer: Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted)).\n",
            "Plain Wrong Answer: Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted)).\n",
            "\n",
            "Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted)).\n",
            "Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted)).\n",
            "Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted)).\n",
            "Wells Fargo net income for 2022"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted).\\nRogue Answer: Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted)).\\nBad Answer: Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted)).\\nMisleading Answer: Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted)).\\nPlain Wrong Answer: Wells Fargo net income for 2022 was $13.2 billion ($3.14 diluted)).\\n\\nWells Fargo net income for 2022 was $13.2 billion ($3.14 diluted)).\\nWells Fargo net income for 2022 was $13.2 billion ($3.14 diluted)).\\nWells Fargo net income for 2022 was $13.2 billion ($3.14 diluted)).\\nWells Fargo net income for 2022'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query1=\"What is the fullform of HTM?\"\n",
        "#query=\"What was the net income of Wells Fargo in 2022?\"\n",
        "docs=docsearch.similarity_search(query1)\n",
        "docs"
      ],
      "metadata": {
        "id": "jfgBu8Mui9Ta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4801e07b-3e73-49bd-b5af-55865695bbfa"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='liquid, high-quality U.S. Treasury and federal agency debt, and agency MBS. The portfolio also includes securities issued by U.S. states and political subdivisions and highly rated CLOs. Debt securities are classified as HTM at the time of purchase or when transferred from the AFS debt securities portfolio. Our intent is to hold these securities to maturity and collect the contractual cash flows. In January 2023, we changed our intent with respect to HTM debt securities with an amortized cost'),\n",
              " Document(page_content='liquid, high-quality U.S. Treasury and federal agency debt, and agency MBS. The portfolio also includes securities issued by U.S. states and political subdivisions and highly rated CLOs. Debt securities are classified as HTM at the time of purchase or when transferred from the AFS debt securities portfolio. Our intent is to hold these securities to maturity and collect the contractual cash flows. In January 2023, we changed our intent with respect to HTM debt securities with an amortized cost'),\n",
              " Document(page_content='liquid, high-quality U.S. Treasury and federal agency debt, and agency MBS. The portfolio also includes securities issued by U.S. states and political subdivisions and highly rated CLOs. Debt securities are classified as HTM at the time of purchase or when transferred from the AFS debt securities portfolio. Our intent is to hold these securities to maturity and collect the contractual cash flows. In January 2023, we changed our intent with respect to HTM debt securities with an amortized cost'),\n",
              " Document(page_content='liquid, high-quality U.S. Treasury and federal agency debt, and agency MBS. The portfolio also includes securities issued by U.S. states and political subdivisions and highly rated CLOs. Debt securities are classified as HTM at the time of purchase or when transferred from the AFS debt securities portfolio. Our intent is to hold these securities to maturity and collect the contractual cash flows. In January 2023, we changed our intent with respect to HTM debt securities with an amortized cost')]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(input_documents=docs, question=query1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "cXOHxPxXi-cq",
        "outputId": "b7789f20-3c5d-4d81-f2c1-35943a4fe262"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " HTM is a term which is used to describe a high-yielding Treasury and Federal Agency Debt Securities. HTM Securities are a very special type of Securities which have a lot of very very special properties which have very very special characteristics which have very special properties which have special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' HTM is a term which is used to describe a high-yielding Treasury and Federal Agency Debt Securities. HTM Securities are a very special type of Securities which have a lot of very very special properties which have very very special characteristics which have very special properties which have special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties which have very special properties'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Query the Docs to get the Answer Back (Hugging Face Model)**bold text**"
      ],
      "metadata": {
        "id": "velHQqEajEfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain.llms import HuggingFaceHub"
      ],
      "metadata": {
        "id": "u7I7Au_QjGNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#llm=HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})"
      ],
      "metadata": {
        "id": "_9_9Z_j4jLDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chain=load_qa_chain(llm, chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "bxUrlkQVjLuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#query=\"What are examples of good data science teams?\"\n",
        "#docs=docsearch.similarity_search(query)"
      ],
      "metadata": {
        "id": "q5tXWGjHjN90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "id": "T3qh2US2jPr9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}